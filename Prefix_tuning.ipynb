{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju-alwbHmKYA",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "!pip install -U adapter-transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYHRQKkM5xhl",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"imdb\")\n",
        "dataset.num_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRaB3OFw7Jkq",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "uHxBuIjFhqzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xdVDIc58O6g",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def encode_batch(batch):\n",
        "  return tokenizer(batch[\"text\"], max_length=80, truncation=True, padding=\"max_length\")\n",
        "\n",
        "dataset = dataset.map(encode_batch, batched=True)\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model\n"
      ],
      "metadata": {
        "id": "aHcYmG0V7QlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FRft_5AAlQd",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import TrainingArguments, AdapterTrainer, EvalPrediction, Trainer\n",
        "from transformers.adapters import PrefixTuningConfig\n",
        "from transformers import  BertModel, AutoModelForSequenceClassification\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_accuracy(p: EvalPrediction):\n",
        "      preds = np.argmax(p.predictions, axis=1)\n",
        "      return {\"acc\": (preds == p.label_ids).mean()}\n",
        "\n",
        "def training(type_model):\n",
        "  config = AutoConfig.from_pretrained(\n",
        "      \"bert-base-uncased\",\n",
        "      num_labels=2,\n",
        "  )\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\n",
        "      \"bert-base-uncased\",\n",
        "      config=config,\n",
        "  )\n",
        "  if(type_model==1):\n",
        "    config = PrefixTuningConfig(flat=False, prefix_length=30)\n",
        "    model.add_adapter(\"prefix_tuning\", config=config)\n",
        "    model.train_adapter(\"prefix_tuning\")\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        learning_rate=1e-4,\n",
        "        num_train_epochs=6,\n",
        "        per_device_train_batch_size=32,\n",
        "        per_device_eval_batch_size=32,\n",
        "        logging_steps=200,\n",
        "        output_dir=\"./training_output\",\n",
        "        overwrite_output_dir=True,\n",
        "        # The next line is important to ensure the dataset labels are properly passed to the model\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    \n",
        "\n",
        "    trainer = AdapterTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        compute_metrics=compute_accuracy,\n",
        "    )\n",
        "\n",
        "  else:\n",
        "    training_args = TrainingArguments(\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    logging_steps=200,\n",
        "    output_dir=\"./training_output\",\n",
        "    overwrite_output_dir=True,)\n",
        "\n",
        "\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[\"test\"],\n",
        "        compute_metrics=compute_accuracy,\n",
        "    )\n",
        "  print(\"fff\")  \n",
        "  return trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcZMwiJ_KDdP",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "trainer = training(0)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PgHWEhsYeMv",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arw8GcsA8MeK",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "classifier = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=training_args.device.index)\n",
        "\n",
        "classifier(\"This is awesome!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfK_AtSz4tlt",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "model.save_adapter(\"./final_adapter\", \"rotten_tomatoes\")\n",
        "\n",
        "!ls -lh final_adapter"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}